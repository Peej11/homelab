---
apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
spec:
  cephConfig:
    global:
      osd_pool_default_size: "1"
      mon_warn_on_pool_no_redundancy: "false"
      bdev_flock_retry: "20"
      bluefs_buffered_io: "false"
      mon_data_avail_warn: "10"
  cephVersion:
    image: quay.io/ceph/ceph:v20.2.0
  dashboard:
    enabled: true
  dataDirHostPath: /var/lib/rook
  mgr:
    count: 3
    allowMultiplePerNode: false
    modules:
      - name: rook
        enabled: true
  mon:
    count: 3
    allowMultiplePerNode: false
  storage:
    allowDeviceClassUpdate: true
    allowOsdCrushWeightUpdate: false
    useAllNodes: true
    useAllDevices: true
    #nodes:
    #  - name: blackwidow
    #    devices:
    #      - name: "/dev/disk/by-id/ata-SAMSUNG_SSD_PM871b_M.2_2280_256GB_S3U0NE0K101064"
    #  - name: captainamerica
    #    devices:
    #      - name: "/dev/disk/by-id/nvme-PC611_NVMe_SK_hynix_256GB_ND0AN50951010A92Y"
    #  - name: hawkeye
    #    devices:
    #      - name: "/dev/disk/by-id/ata-LITEON_L8H-256V2G-11_M.2_2280_256GB_TW0MGNHV550855AGB32L"
    #  - name: ironman
    #    devices:
    #      - name: "/dev/disk/by-id/nvme-THNSN5256GPU7_NVMe_TOSHIBA_256GB__665B31NOKTNX"
    #  - name: thor
    #    devices:
    #      - name: "/dev/disk/by-id/nvme-SAMSUNG_MZVLB256HBHQ-000L7_S4ELNF2M929966"
